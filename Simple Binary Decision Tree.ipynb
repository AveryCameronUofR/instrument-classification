{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions declared\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import tree\n",
    "\n",
    "##########Function version of avery's work. meant to return features once called in BuildDataset. ################\n",
    "def feature_extract(file):\n",
    "    y, sr = librosa.load(file, sr=None)\n",
    "    \n",
    "    hop_length = 512\n",
    "    \n",
    "    # Separate harmonics and percussives into two waveforms\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)    \n",
    "    \n",
    "    #Chroma Energy Normalized (CENS)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    \n",
    "    #Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, \n",
    "                                                 fmax = 8000)\n",
    "    #Mel-Frequency Cepstral Coefficients (MFCC) features from the raw signal\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "    \n",
    "    #Spectral Contrast\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    \n",
    "    y_harmonic = np.mean(y_harmonic)\n",
    "    y_percussive = np.mean(y_percussive)\n",
    "    mel_spec = np.mean(mel_spec, axis=1)\n",
    "    mfcc = np.mean(mfcc, axis =1)\n",
    "    chroma_cens = np.mean(chroma_cens, axis=1)\n",
    "    spec_contrast = np.mean(spec_contrast, axis=1)\n",
    "    \n",
    "    return [y_harmonic, y_percussive, chroma_cens, mfcc, mel_spec, \n",
    "            spec_contrast]\n",
    "\n",
    "def feature_extract_averages(file):\n",
    "    y, sr = librosa.load(file, sr=None)\n",
    "    \n",
    "    hop_length = 512\n",
    "    \n",
    "    # Separate harmonics and percussives into two waveforms\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)    \n",
    "    \n",
    "    #Chroma Energy Normalized (CENS)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    \n",
    "    #Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, \n",
    "                                                 fmax = 8000)\n",
    "    #Mel-Frequency Cepstral Coefficients (MFCC) features from the raw signal\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "    \n",
    "    #Spectral Contrast\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    \n",
    "    y_harmonic = np.mean(y_harmonic)\n",
    "    y_percussive = np.mean(y_percussive)\n",
    "    mel_spec = np.mean(mel_spec)\n",
    "    mfcc = np.mean(mfcc)\n",
    "    chroma_cens = np.mean(chroma_cens)\n",
    "    spec_contrast = np.mean(spec_contrast)\n",
    "    \n",
    "    return [y_harmonic, y_percussive, chroma_cens, mfcc, mel_spec, \n",
    "            spec_contrast]\n",
    "\n",
    "def getDataset(dataGroup, instrument, source):\n",
    "    \n",
    "    new_dir='Dataset/nsynth-'+dataGroup+'/audio/'     #set the audio directory (test, train, etc)\n",
    "    dataframe_raw = pd.read_json(path_or_buf='Dataset/nsynth-'+dataGroup+'/examples.json', orient='index') #read all instruments from examples.json\n",
    "    dataframe_specific = dataframe_raw.loc[dataframe_raw['instrument_family_str'] == instrument]           #narrow down by family (strings, etc)\n",
    "    dataframe_specific = dataframe_specific.loc[dataframe_specific['instrument_source_str'] == source]     #narrow down by source (acoustic, etc)\n",
    "\n",
    "    filenames = dataframe_specific.index.tolist()     #get filenames from our dataframe, put into list\n",
    "    \n",
    "    dictionary = {}\n",
    "    for file in tqdm_notebook(filenames):             #for all files in filenames. Also,  tqdm is a loading bar\n",
    "        features = feature_extract_averages((new_dir+file+'.wav')) #specify directory, file, then add .wav. we will perform feature_extract with the file\n",
    "        dictionary[file] = features                       #make dictionary using file as rows - features as columns\n",
    "    featureDf = pd.DataFrame.from_dict(dictionary, orient='index', #turn into dataframe\n",
    "                                       columns=['y_harmonic', 'y_percussive', 'chroma_cens', \n",
    "                                                'mfcc', 'mel_spec', 'spec_contrast'])\n",
    "    \n",
    "    return featureDf #returns dataframe of features\n",
    "\n",
    "def instrument_code(filename):\n",
    "    \"\"\"\n",
    "    Function that takes in a filename and returns instrument based on naming convention\n",
    "    \"\"\"\n",
    "    class_names=['bass', 'brass', 'flute', 'guitar', \n",
    "             'keyboard', 'mallet', 'organ', 'reed', \n",
    "             'string', 'synth_lead', 'vocal']\n",
    "    \n",
    "    for name in class_names:\n",
    "        if name in filename:\n",
    "            return class_names.index(name)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def equalize_data(class1, class2):\n",
    "    class1_copy = class1\n",
    "    class2_copy = class2  \n",
    "    if (class1_copy['y_harmonic'].count() < class2_copy['y_harmonic'].count()):\n",
    "        while (class1_copy['y_harmonic'].count()<class2_copy['y_harmonic'].count()): \n",
    "            temp = [class1_copy, class1]\n",
    "            class2_copy = pd.concat(temp)\n",
    "        class2_copy = class2_copy[:class1_copy['y_harmonic'].count()]\n",
    "    else:\n",
    "        while (class2_copy['y_harmonic'].count()<class1_copy['y_harmonic'].count()):\n",
    "            temp = [class2_copy, class2]\n",
    "            class2_copy = pd.concat(temp)\n",
    "        class2_copy = class2_copy[:class1_copy['y_harmonic'].count()]\n",
    "    return (class1_copy, class2_copy)\n",
    "\n",
    "print('functions declared')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad44dc27778494eb742d064c07588a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc9400344f94afc99e134bfe74a9ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=119), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CLASS_NAMES=['bass', 'brass', 'flute', 'guitar', \n",
    "             'keyboard', 'mallet', 'organ', 'reed', \n",
    "             'string', 'synth_lead', 'vocal']\n",
    "\n",
    "SOURCE_NAMES=['acoustic', 'electronic', 'synthetic']\n",
    "DATA_GROUPS=['test', 'valid', 'train']\n",
    "\n",
    "dataGroup = DATA_GROUPS[0]    #SET IF YOU WANT TEST, TRAIN, OR VALID (IF YOU HAVE IT)\n",
    "source = SOURCE_NAMES[0]      #SET ACOUSTIC, ELECTRONIC, SYNTHETIC\n",
    "\n",
    "# get string members from dataset\n",
    "string_df = getDataset(dataGroup, 'string', source)\n",
    "\n",
    "# get keyboard members\n",
    "keyboard_df = getDataset(dataGroup, 'keyboard', source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add targets to dataframes\n",
    "string_targets_test = []\n",
    "keyboard_targets_test = []\n",
    "\n",
    "for name in string_df.index.tolist():\n",
    "    string_targets_test.append(instrument_code(name))\n",
    "    \n",
    "string_df['target'] = string_targets_test\n",
    "\n",
    "for name in keyboard_df.index.tolist():\n",
    "    keyboard_targets_test.append(instrument_code(name))\n",
    "\n",
    "keyboard_df['target'] = keyboard_targets_test\n",
    "\n",
    "# balance the dataset\n",
    "(string_df, keyboard_df) = equalize_data(string_df, keyboard_df)\n",
    "input_df = pd.concat([string_df, keyboard_df])\n",
    "\n",
    "# randomize dataset\n",
    "input_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x = input_df[['y_harmonic', 'y_percussive', 'chroma_cens', 'mfcc', 'mel_spec', 'spec_contrast']].values.tolist()\n",
    "\n",
    "# encoding target as 0 for keyboard, 1 for string\n",
    "y = input_df['target'].map(lambda target: 0 if (target == 4) else 1).values.tolist()\n",
    "\n",
    "# create a scikit-learn tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x, y)\n",
    "\n",
    "print([y[0], y[1], y[2], y[3], y[4], y[5]])\n",
    "clf.predict([x[0], x[1], x[2], x[3], x[4], x[5]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
